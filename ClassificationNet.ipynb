{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "npGiuvftxf1z",
    "outputId": "48a8cb76-f300-4db3-f216-eba77d4cce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.18</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import itertools\n",
    "import pixiedust\n",
    "import random\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "plt.ion()   # interactive mode\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "MHoa2aySwBF_",
    "outputId": "31d7f712-1312-4029-8773-9d4b77d44312"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path ('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### sanity check for the images \n",
    "# classes = ['wazers', 'waze_logo', 'non']\n",
    "# for label in classes:\n",
    "#     print(\"Class =\",label)\n",
    "#     !ls $DATA_DIR\\VAL\\$emotion | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRBBa4e-xRBD"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# for validatin we use normalization and resize (for train we also change the angle and size of the images)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hmElu6Slwjgf",
    "outputId": "6b438b4a-4913-46c6-9bc5-6884fcfa2d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "Train image size: 16844\n",
      "Validation image size: 5296\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "''' The function takes the data loader and a parameter  '''\n",
    "def create_train_val_slice(image_datasets,sample_size=None,val_same_as_train=False):\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    \n",
    "    if not sample_size: # return the whole data\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                                      shuffle=True, num_workers=1)\n",
    "                      for x in ['train', 'val']}\n",
    "        return dataloaders, dataset_sizes\n",
    "    \n",
    "    sample_n = {x: random.sample(list(range(dataset_sizes[x])), sample_size)\n",
    "                for x in ['train', 'val']}\n",
    "\n",
    "    image_datasets_reduced = {x: torch.utils.data.Subset(image_datasets['train' if val_same_as_train else x], sample_n['train'])\n",
    "                              for x in ['train', 'val']}\n",
    "    dataset_sizes = {x: len(image_datasets_reduced[x]) for x in ['train', 'val']}\n",
    "\n",
    "    dataloaders_reduced = {x: torch.utils.data.DataLoader(image_datasets_reduced[x], batch_size=BATCH_SIZE,\n",
    "                                                  shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "    return dataloaders_reduced, dataset_sizes\n",
    "\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "#sample_size = 100\n",
    "#data, dataset_sizes =  create_train_val_slice(image_datasets,sample_size,True)\n",
    "data, dataset_sizes =  create_train_val_slice(image_datasets,None)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Classes: \", class_names) \n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "aOMEb5deCofk",
    "outputId": "40524968-328f-420d-cfc1-1e3fe2632a01"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "# # Make a grid from batch\n",
    "# sample_train_images = torchvision.utils.make_grid(inputs)\n",
    "# #imshow(sample_train_images, title=classes)\n",
    "# print(f\"classes={classes}\")\n",
    "# imshow(sample_train_images, title=[class_names[i] for i in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     25,
     27,
     79
    ],
    "colab": {},
    "colab_type": "code",
    "id": "sDNvKgitDNG7",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data, model, criterion, optimizer, scheduler, num_epochs=2, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print('Val loss: {}, Val accuracy: {}'\\\n",
    "              .format(checkpoint[\"best_val_loss\"], checkpoint[\"best_val_accuracy\"]))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "           \n",
    "    print(\"Starting epochs\")\n",
    "    outer = tqdm(total=num_epochs, desc='Epoch', position=0)\n",
    "    inner = tqdm(total=(dataset_sizes['train']//BATCH_SIZE), position=1)\n",
    "    for epoch in range(num_epochs):\n",
    "        outer.update(1)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            total = dataset_sizes[phase]//BATCH_SIZE\n",
    "\n",
    "            # Handle tqdm inner loop counter\n",
    "            inner.total = dataset_sizes[phase]//BATCH_SIZE\n",
    "            inner.reset()\n",
    "            inner_total = 0 \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i>0:\n",
    "                    report_str = f'[{epoch + 1}, {i}] loss: {(running_loss / i * inputs.size(0)):.3f}'\n",
    "                else:\n",
    "                    report_str = \"first iteration\"\n",
    "\n",
    "                # Update inner tqdm, we are about to override the previous maximum, update maximum\n",
    "                if inner_total >= total:\n",
    "                    total = total *2\n",
    "                    inner.total = total\n",
    "                inner_total = inner_total + 1\n",
    "                inner.update(1) # Advance the tqdm counter\n",
    "                inner.desc = f'Phase: {phase} ' + report_str\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                # print(\"running_corrects =\", running_corrects)\n",
    "                \n",
    "            if phase == 'train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "            #inner.write(\"running_corrects=\", running_corrects, \" epoch: \", epoch)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            loss_str = f'Epoch: {epoch+1} of {num_epochs}, {phase:6} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}'\n",
    "            inner.write(loss_str)\n",
    "            \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                inner.write('New best model found!')\n",
    "                inner.write(f'New record loss:{epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                #save the weights\n",
    "                torch.save(best_model_wts, CHECK_POINT_PATH)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "3876f5ef3ff947b7b26c72b6512166f8",
      "79f9ecfbdd9a4871b95ae9668720c847",
      "fa2dc4675ff940b48a90425981eb55a6",
      "2e81eb8a8cdb40738fb85ce51cfed6c7",
      "966b7f8371654dd2a54f484c4a9c0129",
      "a315579ab89f43e2b7d184c7c4d64baa",
      "6d2c8e3068854041be63a92194b35a64",
      "08e0b965fc614feb960cf954223db3e3"
     ]
    },
    "colab_type": "code",
    "id": "AUF-cgM1y_7L",
    "outputId": "107315c6-103a-4192-a611-d3e76809d530"
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "#model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "#model_conv = torchvision.models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_conv.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35uVilmuNgg-"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UgElP8XDrYm"
   },
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "ct = 0\n",
    "for child in model_conv.children():\n",
    "  ct += 1\n",
    "  # freezes layers 1-6 in the total 10 layers of Resnet50\n",
    "  if ct < 7:\n",
    "    for param in child.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "'''two options to write the loss. They are both equal'''\n",
    "# option 1 #\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# option 2 #\n",
    "# p = nn.functional.softmax(model_conv, dim=1)\n",
    "# # to calculate loss using probabilities you can do below \n",
    "# criterion = nn.functional.nll_loss(torch.log(p), y)\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=70, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IaCCoou62TKD",
    "outputId": "058f59dd-7db0-4456-f29d-42ae2d4567c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = LABS_DIR/'ModelParams'/'checkpoint.tar'\n",
    "\n",
    "# !del $CHECK_POINT_PATH\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_tKaq9vU0cjq",
    "outputId": "1b3c5122-2b8f-4c1c-cedc-811cbfe8fcfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd335522fa54d59b5adb2c1bed354e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', style=ProgressStyle(description_width='initial'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34de7e4856bc44748f4a47520cbe5b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1052.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 100, train  Loss: 2.4186 Acc: 0.3159\n",
      "Epoch: 1 of 100, val    Loss: 2.3710 Acc: 0.3578\n",
      "New best model found!\n",
      "New record loss:2.370964116920517, previous record loss: inf\n",
      "\n",
      "Epoch: 2 of 100, train  Loss: 2.4900 Acc: 0.3400\n",
      "Epoch: 2 of 100, val    Loss: 2.6435 Acc: 0.3820\n",
      "\n",
      "Epoch: 3 of 100, train  Loss: 2.5359 Acc: 0.3478\n",
      "Epoch: 3 of 100, val    Loss: 2.1491 Acc: 0.4037\n",
      "New best model found!\n",
      "New record loss:2.149141051978143, previous record loss: 2.370964116920517\n",
      "\n",
      "Epoch: 4 of 100, train  Loss: 2.5459 Acc: 0.3453\n",
      "Epoch: 4 of 100, val    Loss: 3.9535 Acc: 0.3691\n",
      "\n",
      "Epoch: 5 of 100, train  Loss: 2.4869 Acc: 0.3459\n",
      "Epoch: 5 of 100, val    Loss: 2.1032 Acc: 0.3648\n",
      "New best model found!\n",
      "New record loss:2.1031726919272153, previous record loss: 2.149141051978143\n",
      "\n",
      "Epoch: 6 of 100, train  Loss: 2.5056 Acc: 0.3484\n",
      "Epoch: 6 of 100, val    Loss: 2.4611 Acc: 0.3865\n",
      "\n",
      "Epoch: 7 of 100, train  Loss: 2.4752 Acc: 0.3475\n",
      "Epoch: 7 of 100, val    Loss: 2.6406 Acc: 0.3710\n",
      "\n",
      "Epoch: 8 of 100, train  Loss: 2.4426 Acc: 0.3537\n",
      "Epoch: 8 of 100, val    Loss: 2.5396 Acc: 0.3627\n",
      "\n",
      "Epoch: 9 of 100, train  Loss: 2.4305 Acc: 0.3503\n",
      "Epoch: 9 of 100, val    Loss: 2.7129 Acc: 0.3639\n",
      "\n",
      "Epoch: 10 of 100, train  Loss: 2.5161 Acc: 0.3468\n",
      "Epoch: 10 of 100, val    Loss: 2.4046 Acc: 0.3404\n",
      "\n",
      "Epoch: 11 of 100, train  Loss: 2.4848 Acc: 0.3476\n",
      "Epoch: 11 of 100, val    Loss: 2.8633 Acc: 0.3123\n",
      "\n",
      "Epoch: 12 of 100, train  Loss: 2.4123 Acc: 0.3547\n",
      "Epoch: 12 of 100, val    Loss: 1.9983 Acc: 0.3886\n",
      "New best model found!\n",
      "New record loss:1.9982754539507033, previous record loss: 2.1031726919272153\n",
      "\n",
      "Epoch: 13 of 100, train  Loss: 2.4924 Acc: 0.3469\n",
      "Epoch: 13 of 100, val    Loss: 2.8967 Acc: 0.3618\n",
      "\n",
      "Epoch: 14 of 100, train  Loss: 2.5021 Acc: 0.3505\n",
      "Epoch: 14 of 100, val    Loss: 3.1467 Acc: 0.3014\n",
      "\n",
      "Epoch: 15 of 100, train  Loss: 2.4921 Acc: 0.3465\n",
      "Epoch: 15 of 100, val    Loss: 2.5451 Acc: 0.3338\n",
      "\n",
      "Epoch: 16 of 100, train  Loss: 2.4658 Acc: 0.3483\n",
      "Epoch: 16 of 100, val    Loss: 2.3515 Acc: 0.3631\n",
      "\n",
      "Epoch: 17 of 100, train  Loss: 2.3905 Acc: 0.3471\n",
      "Epoch: 17 of 100, val    Loss: 2.1651 Acc: 0.3712\n",
      "\n",
      "Epoch: 18 of 100, train  Loss: 2.4157 Acc: 0.3515\n",
      "Epoch: 18 of 100, val    Loss: 2.2108 Acc: 0.3578\n",
      "\n",
      "Epoch: 19 of 100, train  Loss: 2.4971 Acc: 0.3511\n",
      "Epoch: 19 of 100, val    Loss: 2.6865 Acc: 0.3765\n",
      "\n",
      "Epoch: 20 of 100, train  Loss: 2.5541 Acc: 0.3475\n",
      "Epoch: 20 of 100, val    Loss: 2.8786 Acc: 0.2812\n",
      "\n",
      "Epoch: 21 of 100, train  Loss: 2.5029 Acc: 0.3503\n",
      "Epoch: 21 of 100, val    Loss: 3.3629 Acc: 0.3478\n",
      "\n",
      "Epoch: 22 of 100, train  Loss: 2.4836 Acc: 0.3500\n",
      "Epoch: 22 of 100, val    Loss: 2.1189 Acc: 0.4045\n",
      "\n",
      "Epoch: 23 of 100, train  Loss: 2.4277 Acc: 0.3599\n",
      "Epoch: 23 of 100, val    Loss: 2.4131 Acc: 0.3848\n",
      "\n",
      "Epoch: 24 of 100, train  Loss: 2.4612 Acc: 0.3513\n",
      "Epoch: 24 of 100, val    Loss: 2.1098 Acc: 0.4041\n",
      "\n",
      "Epoch: 25 of 100, train  Loss: 2.4603 Acc: 0.3505\n",
      "Epoch: 25 of 100, val    Loss: 2.3363 Acc: 0.3965\n",
      "\n",
      "Epoch: 26 of 100, train  Loss: 2.4753 Acc: 0.3513\n",
      "Epoch: 26 of 100, val    Loss: 2.4417 Acc: 0.3850\n",
      "\n",
      "Epoch: 27 of 100, train  Loss: 2.4272 Acc: 0.3490\n",
      "Epoch: 27 of 100, val    Loss: 2.7160 Acc: 0.2895\n",
      "\n",
      "Epoch: 28 of 100, train  Loss: 2.4554 Acc: 0.3493\n",
      "Epoch: 28 of 100, val    Loss: 2.3887 Acc: 0.3880\n",
      "\n",
      "Epoch: 29 of 100, train  Loss: 2.3937 Acc: 0.3531\n",
      "Epoch: 29 of 100, val    Loss: 2.7275 Acc: 0.3404\n",
      "\n",
      "Epoch: 30 of 100, train  Loss: 2.5572 Acc: 0.3491\n",
      "Epoch: 30 of 100, val    Loss: 2.5691 Acc: 0.3308\n",
      "\n",
      "Epoch: 31 of 100, train  Loss: 2.5795 Acc: 0.3457\n",
      "Epoch: 31 of 100, val    Loss: 2.7213 Acc: 0.3707\n",
      "\n",
      "Epoch: 32 of 100, train  Loss: 2.4800 Acc: 0.3525\n",
      "Epoch: 32 of 100, val    Loss: 2.1786 Acc: 0.3978\n",
      "\n",
      "Epoch: 33 of 100, train  Loss: 2.5729 Acc: 0.3463\n",
      "Epoch: 33 of 100, val    Loss: 3.4067 Acc: 0.3814\n",
      "\n",
      "Epoch: 34 of 100, train  Loss: 2.5491 Acc: 0.3542\n",
      "Epoch: 34 of 100, val    Loss: 3.8568 Acc: 0.3261\n",
      "\n",
      "Epoch: 35 of 100, train  Loss: 2.4385 Acc: 0.3524\n",
      "Epoch: 35 of 100, val    Loss: 2.3291 Acc: 0.3790\n",
      "\n",
      "Epoch: 36 of 100, train  Loss: 2.5268 Acc: 0.3467\n",
      "Epoch: 36 of 100, val    Loss: 2.7017 Acc: 0.3457\n",
      "\n",
      "Epoch: 37 of 100, train  Loss: 2.4641 Acc: 0.3487\n",
      "Epoch: 37 of 100, val    Loss: 2.5606 Acc: 0.3537\n",
      "\n",
      "Epoch: 38 of 100, train  Loss: 2.4353 Acc: 0.3575\n",
      "Epoch: 38 of 100, val    Loss: 2.9160 Acc: 0.3080\n",
      "\n",
      "Epoch: 39 of 100, train  Loss: 2.3682 Acc: 0.3535\n",
      "Epoch: 39 of 100, val    Loss: 2.9901 Acc: 0.3493\n",
      "\n",
      "Epoch: 40 of 100, train  Loss: 2.4776 Acc: 0.3528\n",
      "Epoch: 40 of 100, val    Loss: 2.2541 Acc: 0.3412\n",
      "\n",
      "Epoch: 41 of 100, train  Loss: 2.4346 Acc: 0.3509\n",
      "Epoch: 41 of 100, val    Loss: 2.6688 Acc: 0.3151\n",
      "\n",
      "Epoch: 42 of 100, train  Loss: 2.4365 Acc: 0.3540\n",
      "Epoch: 42 of 100, val    Loss: 2.0389 Acc: 0.3926\n",
      "\n",
      "Epoch: 43 of 100, train  Loss: 2.5403 Acc: 0.3450\n",
      "Epoch: 43 of 100, val    Loss: 2.3684 Acc: 0.3865\n",
      "\n",
      "Epoch: 44 of 100, train  Loss: 2.4541 Acc: 0.3531\n",
      "Epoch: 44 of 100, val    Loss: 2.5414 Acc: 0.3314\n",
      "\n",
      "Epoch: 45 of 100, train  Loss: 2.5403 Acc: 0.3430\n",
      "Epoch: 45 of 100, val    Loss: 3.5526 Acc: 0.2708\n",
      "\n",
      "Epoch: 46 of 100, train  Loss: 2.3824 Acc: 0.3562\n",
      "Epoch: 46 of 100, val    Loss: 2.5070 Acc: 0.3697\n",
      "\n",
      "Epoch: 47 of 100, train  Loss: 2.5789 Acc: 0.3414\n",
      "Epoch: 47 of 100, val    Loss: 2.3423 Acc: 0.3861\n",
      "\n",
      "Epoch: 48 of 100, train  Loss: 2.4192 Acc: 0.3563\n",
      "Epoch: 48 of 100, val    Loss: 2.8406 Acc: 0.3593\n",
      "\n",
      "Epoch: 49 of 100, train  Loss: 2.5494 Acc: 0.3475\n",
      "Epoch: 49 of 100, val    Loss: 2.7164 Acc: 0.2965\n",
      "\n",
      "Epoch: 50 of 100, train  Loss: 2.4764 Acc: 0.3554\n",
      "Epoch: 50 of 100, val    Loss: 2.3921 Acc: 0.3616\n",
      "\n",
      "Epoch: 51 of 100, train  Loss: 2.4846 Acc: 0.3480\n",
      "Epoch: 51 of 100, val    Loss: 2.5777 Acc: 0.3833\n",
      "\n",
      "Epoch: 52 of 100, train  Loss: 2.6187 Acc: 0.3462\n",
      "Epoch: 52 of 100, val    Loss: 2.5522 Acc: 0.4024\n",
      "\n",
      "Epoch: 53 of 100, train  Loss: 2.4539 Acc: 0.3507\n",
      "Epoch: 53 of 100, val    Loss: 2.2250 Acc: 0.3795\n",
      "\n",
      "Epoch: 54 of 100, train  Loss: 2.4440 Acc: 0.3511\n",
      "Epoch: 54 of 100, val    Loss: 3.2152 Acc: 0.3015\n",
      "\n",
      "Epoch: 55 of 100, train  Loss: 2.4751 Acc: 0.3461\n",
      "Epoch: 55 of 100, val    Loss: 3.1180 Acc: 0.3812\n",
      "\n",
      "Epoch: 56 of 100, train  Loss: 2.5467 Acc: 0.3526\n",
      "Epoch: 56 of 100, val    Loss: 2.4458 Acc: 0.3342\n",
      "\n",
      "Epoch: 57 of 100, train  Loss: 2.5527 Acc: 0.3417\n",
      "Epoch: 57 of 100, val    Loss: 2.1767 Acc: 0.3731\n",
      "\n",
      "Epoch: 58 of 100, train  Loss: 2.5939 Acc: 0.3423\n",
      "Epoch: 58 of 100, val    Loss: 2.7397 Acc: 0.3786\n",
      "\n",
      "Epoch: 59 of 100, train  Loss: 2.5611 Acc: 0.3459\n",
      "Epoch: 59 of 100, val    Loss: 3.4545 Acc: 0.3361\n",
      "\n",
      "Epoch: 60 of 100, train  Loss: 2.4548 Acc: 0.3528\n",
      "Epoch: 60 of 100, val    Loss: 2.6132 Acc: 0.3146\n",
      "\n",
      "Epoch: 61 of 100, train  Loss: 2.4322 Acc: 0.3546\n",
      "Epoch: 61 of 100, val    Loss: 2.5491 Acc: 0.3746\n",
      "\n",
      "Epoch: 62 of 100, train  Loss: 2.5941 Acc: 0.3461\n",
      "Epoch: 62 of 100, val    Loss: 3.2884 Acc: 0.3382\n",
      "\n",
      "Epoch: 63 of 100, train  Loss: 2.5184 Acc: 0.3512\n",
      "Epoch: 63 of 100, val    Loss: 2.5197 Acc: 0.3478\n",
      "\n",
      "Epoch: 64 of 100, train  Loss: 2.5733 Acc: 0.3467\n",
      "Epoch: 64 of 100, val    Loss: 3.2046 Acc: 0.3163\n",
      "\n",
      "Epoch: 65 of 100, train  Loss: 2.5431 Acc: 0.3476\n",
      "Epoch: 65 of 100, val    Loss: 2.1347 Acc: 0.4056\n",
      "\n",
      "Epoch: 66 of 100, train  Loss: 2.5209 Acc: 0.3560\n",
      "Epoch: 66 of 100, val    Loss: 2.2203 Acc: 0.3943\n",
      "\n",
      "Epoch: 67 of 100, train  Loss: 2.3815 Acc: 0.3583\n",
      "Epoch: 67 of 100, val    Loss: 2.9996 Acc: 0.3856\n",
      "\n",
      "Epoch: 68 of 100, train  Loss: 2.5770 Acc: 0.3451\n",
      "Epoch: 68 of 100, val    Loss: 2.5295 Acc: 0.3801\n",
      "\n",
      "Epoch: 69 of 100, train  Loss: 2.3791 Acc: 0.3541\n",
      "Epoch: 69 of 100, val    Loss: 2.1975 Acc: 0.3792\n",
      "\n",
      "Epoch: 70 of 100, train  Loss: 2.5477 Acc: 0.3507\n",
      "Epoch: 70 of 100, val    Loss: 3.5080 Acc: 0.3097\n",
      "\n",
      "Epoch: 71 of 100, train  Loss: 1.6396 Acc: 0.4199\n",
      "Epoch: 71 of 100, val    Loss: 1.7358 Acc: 0.4277\n",
      "New best model found!\n",
      "New record loss:1.7357507319248695, previous record loss: 1.9982754539507033\n",
      "\n",
      "Epoch: 72 of 100, train  Loss: 1.5249 Acc: 0.4295\n",
      "Epoch: 72 of 100, val    Loss: 1.6751 Acc: 0.4320\n",
      "New best model found!\n",
      "New record loss:1.675098431434516, previous record loss: 1.7357507319248695\n",
      "\n",
      "Epoch: 73 of 100, train  Loss: 1.5018 Acc: 0.4323\n",
      "Epoch: 73 of 100, val    Loss: 1.6984 Acc: 0.4345\n",
      "\n",
      "Epoch: 74 of 100, train  Loss: 1.4744 Acc: 0.4381\n",
      "Epoch: 74 of 100, val    Loss: 1.7567 Acc: 0.4122\n",
      "\n",
      "Epoch: 75 of 100, train  Loss: 1.4771 Acc: 0.4339\n",
      "Epoch: 75 of 100, val    Loss: 1.7682 Acc: 0.4103\n",
      "\n",
      "Epoch: 76 of 100, train  Loss: 1.4746 Acc: 0.4314\n",
      "Epoch: 76 of 100, val    Loss: 1.6426 Acc: 0.4256\n",
      "New best model found!\n",
      "New record loss:1.642570775981396, previous record loss: 1.675098431434516\n",
      "\n",
      "Epoch: 77 of 100, train  Loss: 1.4729 Acc: 0.4337\n",
      "Epoch: 77 of 100, val    Loss: 1.6755 Acc: 0.4284\n",
      "\n",
      "Epoch: 78 of 100, train  Loss: 1.4672 Acc: 0.4333\n",
      "Epoch: 78 of 100, val    Loss: 1.6867 Acc: 0.4243\n",
      "\n",
      "Epoch: 79 of 100, train  Loss: 1.4765 Acc: 0.4299\n",
      "Epoch: 79 of 100, val    Loss: 1.6460 Acc: 0.4305\n",
      "\n",
      "Epoch: 80 of 100, train  Loss: 1.4746 Acc: 0.4250\n",
      "Epoch: 80 of 100, val    Loss: 1.6359 Acc: 0.4335\n",
      "New best model found!\n",
      "New record loss:1.6359408047624224, previous record loss: 1.642570775981396\n",
      "\n",
      "Epoch: 81 of 100, train  Loss: 1.4559 Acc: 0.4318\n",
      "Epoch: 81 of 100, val    Loss: 1.6425 Acc: 0.4247\n",
      "\n",
      "Epoch: 82 of 100, train  Loss: 1.4540 Acc: 0.4423\n",
      "Epoch: 82 of 100, val    Loss: 1.6567 Acc: 0.4305\n",
      "\n",
      "Epoch: 83 of 100, train  Loss: 1.4552 Acc: 0.4360\n",
      "Epoch: 83 of 100, val    Loss: 1.6413 Acc: 0.4301\n",
      "\n",
      "Epoch: 84 of 100, train  Loss: 1.4643 Acc: 0.4334\n",
      "Epoch: 84 of 100, val    Loss: 1.6790 Acc: 0.4124\n",
      "\n",
      "Epoch: 85 of 100, train  Loss: 1.4539 Acc: 0.4345\n",
      "Epoch: 85 of 100, val    Loss: 1.6156 Acc: 0.4439\n",
      "New best model found!\n",
      "New record loss:1.6156477171848909, previous record loss: 1.6359408047624224\n",
      "\n",
      "Epoch: 86 of 100, train  Loss: 1.4557 Acc: 0.4367\n",
      "Epoch: 86 of 100, val    Loss: 1.7122 Acc: 0.4158\n",
      "\n",
      "Epoch: 87 of 100, train  Loss: 1.4610 Acc: 0.4304\n",
      "Epoch: 87 of 100, val    Loss: 1.6435 Acc: 0.4307\n",
      "\n",
      "Epoch: 88 of 100, train  Loss: 1.4473 Acc: 0.4412\n",
      "Epoch: 88 of 100, val    Loss: 1.6494 Acc: 0.4243\n",
      "\n",
      "Epoch: 89 of 100, train  Loss: 1.4574 Acc: 0.4327\n",
      "Epoch: 89 of 100, val    Loss: 1.6039 Acc: 0.4407\n",
      "New best model found!\n",
      "New record loss:1.6039253676765994, previous record loss: 1.6156477171848909\n",
      "\n",
      "Epoch: 90 of 100, train  Loss: 1.4528 Acc: 0.4337\n",
      "Epoch: 90 of 100, val    Loss: 1.6189 Acc: 0.4388\n",
      "\n",
      "Epoch: 91 of 100, train  Loss: 1.4605 Acc: 0.4313\n",
      "Epoch: 91 of 100, val    Loss: 1.6289 Acc: 0.4288\n",
      "\n",
      "Epoch: 92 of 100, train  Loss: 1.4545 Acc: 0.4332\n",
      "Epoch: 92 of 100, val    Loss: 1.6635 Acc: 0.4309\n",
      "\n",
      "Epoch: 93 of 100, train  Loss: 1.4512 Acc: 0.4375\n",
      "Epoch: 93 of 100, val    Loss: 1.6638 Acc: 0.4248\n",
      "\n",
      "Epoch: 94 of 100, train  Loss: 1.4506 Acc: 0.4365\n",
      "Epoch: 94 of 100, val    Loss: 1.6362 Acc: 0.4315\n",
      "\n",
      "Epoch: 95 of 100, train  Loss: 1.4485 Acc: 0.4398\n",
      "Epoch: 95 of 100, val    Loss: 1.6507 Acc: 0.4367\n",
      "\n",
      "Epoch: 96 of 100, train  Loss: 1.4561 Acc: 0.4352\n",
      "Epoch: 96 of 100, val    Loss: 1.6393 Acc: 0.4307\n",
      "\n",
      "Epoch: 97 of 100, train  Loss: 1.4536 Acc: 0.4342\n",
      "Epoch: 97 of 100, val    Loss: 1.6253 Acc: 0.4273\n",
      "\n",
      "Epoch: 98 of 100, train  Loss: 1.4390 Acc: 0.4397\n",
      "Epoch: 98 of 100, val    Loss: 1.6262 Acc: 0.4328\n",
      "\n",
      "Epoch: 99 of 100, train  Loss: 1.4471 Acc: 0.4349\n",
      "Epoch: 99 of 100, val    Loss: 1.6010 Acc: 0.4435\n",
      "New best model found!\n",
      "New record loss:1.600962553679763, previous record loss: 1.6039253676765994\n",
      "\n",
      "Epoch: 100 of 100, train  Loss: 1.4495 Acc: 0.4356\n",
      "Epoch: 100 of 100, val    Loss: 1.6104 Acc: 0.4343\n",
      "\n",
      "Training complete in 109m 19s\n",
      "Best val Acc: 0.4435 Best val loss: 1.6010\n",
      "Wall time: 1h 49min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_conv, best_val_loss, best_val_acc = train_model(data,\n",
    "                                                      model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs = 100,\n",
    "                                                      checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3WHcYME2huF"
   },
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQuhWHVqVJE_"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ase07EAFAHwO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Happy': 4627, 'Neutral': 3148, 'Sad': 3058, 'Fear': 2344, 'Angry': 2225, 'Surprise': 1442})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "x='train'\n",
    "d = datasets.ImageFolder(os.path.join(DATA_DIR, x))\n",
    "cnt = Counter([])\n",
    "for i,(image,category) in enumerate(d):\n",
    "    cnt.update({(image_datasets['train'].classes)[category]:1})\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angry'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'].classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NetWithDataLoader.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "output_auto_scroll": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08e0b965fc614feb960cf954223db3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e81eb8a8cdb40738fb85ce51cfed6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e0b965fc614feb960cf954223db3e3",
      "placeholder": "​",
      "style": "IPY_MODEL_6d2c8e3068854041be63a92194b35a64",
      "value": " 44.7M/44.7M [11:04&lt;00:00, 70.5kB/s]"
     }
    },
    "3876f5ef3ff947b7b26c72b6512166f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa2dc4675ff940b48a90425981eb55a6",
       "IPY_MODEL_2e81eb8a8cdb40738fb85ce51cfed6c7"
      ],
      "layout": "IPY_MODEL_79f9ecfbdd9a4871b95ae9668720c847"
     }
    },
    "6d2c8e3068854041be63a92194b35a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79f9ecfbdd9a4871b95ae9668720c847": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "966b7f8371654dd2a54f484c4a9c0129": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a315579ab89f43e2b7d184c7c4d64baa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2dc4675ff940b48a90425981eb55a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a315579ab89f43e2b7d184c7c4d64baa",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966b7f8371654dd2a54f484c4a9c0129",
      "value": 46827520
     }
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}